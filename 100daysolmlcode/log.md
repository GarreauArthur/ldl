11

Goal : work at least an hour a day on AI/ML/DL/Coding/Project.
My goals in AI are :

* learn more models (RNN, GAN, RL, Trees)
* gain more practical experience
* win at least one kaggle competition

# days -2: 2019-01-01

* Watch last videos of deeplearning.ai course 2

# day -1: 2019-01-02

* Getting use to kaggle and learning tensorflow
* Titanic: Machine Learning from Disaster
* <https://www.kaggle.com/cqncpdp/one-sinking-boy>

# day 1: 2019-01-03

* Pledging
* starting this log
* Python Data Science Handbook pages 47 to 58
* bash script (nl.sh) to easily add new entries to this log

# day 2: 2019-01-04

* fix small bug of bash script
* Python Data Science Handbook pages 59 to 62 (lol, i feel sick)

Because Random forest seems to be commonly used in kaggle competition, and

* Learning about and Taking notes on Decision trees : 
  * part 1: <https://youtu.be/7VeUPuFGJHk>
  * part 2: <https://youtu.be/wpNl-JwwplA>
* Learning about Random forest : part 1 <https://youtu.be/J4Wdy0Wc_xQ>

# day 3: 2019-01-05

* Learning about Random forest :
  * end of part 1 <https://youtu.be/J4Wdy0Wc_xQ>
  * part 2 <https://youtu.be/nyxTdL_4Q-Q>

# day 4: 2019-01-06

* i've waste a little bit of time, trying to calculate some probability for a game
* red : https://www.kaggle.com/dansbecker/random-forests
* done the kaggle exercice on random forests, but it was not very helpful. I
need to implement one from scratch. Scikit-learn is cool and all but, I want to
do it the hard way
* red my notes on decision trees

# day 5: 2019-01-07

* Today, I am tired and angry, but Basically I Do Work
* Watching again :
	* deeplearning.ai C5W1L01 : why sequence models
	* deeplearning.ai C5W1L02 : notation
	* deeplearning.ai C5W1L03 : RNN Model
* deeplearning.ai C5W1L04 : Backpropagation through time

# day 6: 2019-01-08

* not very productive today
* deeplearning.ai C5W1L05 : Different types of RNNs
* Start of C5W1L06


# day 7: 2019-01-09

* C4W1L06 : Language model and sequence generation
* C5W1L07 : Sampling novel sequences
* C5W1L08 : Vanishing gradients with RNNs
* StatQuest videos
	* Bias and Variance
	* The confusion matrix
* start implementing decision trees

# day 8: 2019-01-10

* Continue to work on decision trees, it's more difficult than I expected because
I want to deal with numerics, multiple choices, ranks, and binary data. I need
to think a little bit more, before writing code
* C5W1L09 : Gated Recurrent Unit (first 11 minutes)

# day 9: 2019-01-11

* C5W1L09 : Gated Recurrent Unit 
* productivity = 0
* i've read a few pages of the book "Deep Learning" chapter 10
* copied coursera exercises on sequence models

# day 10: 2019-01-12

* keno project : bob wants to win money playing keno, and thinks there's a pattern
so I think it can actually be a funny project to apply my freshly learned knowledge
on RNNs
  * gathering data : found a website with all the results
  * tryed to install a python package to web scrap the data, but it failed
because I messed up with my environements, and really should try to clean
everything up, but I am not ready to spend time doing that
  * I probably can do that with some CLTools, probably can just use awk, I need
to learn awk. Yeah, bbut let use a better tool for the job, xpath. I know
xpath, I don't remember the syntax, because my memory is leaking real bad, so
back to my college material. Actually this file has information on Trees, need
to read it, to help me implement the random forest. That's crazy how fast I
forget things.
I like to learn about things before using them, I think the time I spend learning
helps me being more productive and losing less time, but I am still not sure
about how much time learning is too much.
I need to stop thinking and just go in there, flying, dominating, with speed,
momentum and violence
let's just fucking use javascript, console.log and don't care about anything
Done, what is wrong with me, it's 4:20 am

# day 11: 2019-01-13

* tested the csv file I created this night
* create git repo for the Keno project
