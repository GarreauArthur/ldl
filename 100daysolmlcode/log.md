55

Goal : work at least an hour a day on AI/ML/DL/Coding/Project.
My goals in AI are :

* learn more models (RNN, GAN, RL, Trees)
* gain more practical experience
* win at least one kaggle competition

# days -2: 2019-01-01

* Watch last videos of deeplearning.ai course 2

# day -1: 2019-01-02

* Getting use to kaggle and learning tensorflow
* Titanic: Machine Learning from Disaster
* <https://www.kaggle.com/cqncpdp/one-sinking-boy>

# day 1: 2019-01-03

* Pledging
* starting this log
* Python Data Science Handbook pages 47 to 58
* bash script (nl.sh) to easily add new entries to this log

# day 2: 2019-01-04

* fix small bug of bash script
* Python Data Science Handbook pages 59 to 62 (lol, i feel sick)

Because Random forest seems to be commonly used in kaggle competition, and

* Learning about and Taking notes on Decision trees : 
  * part 1: <https://youtu.be/7VeUPuFGJHk>
  * part 2: <https://youtu.be/wpNl-JwwplA>
* Learning about Random forest : part 1 <https://youtu.be/J4Wdy0Wc_xQ>

# day 3: 2019-01-05

* Learning about Random forest :
  * end of part 1 <https://youtu.be/J4Wdy0Wc_xQ>
  * part 2 <https://youtu.be/nyxTdL_4Q-Q>

# day 4: 2019-01-06

* i've waste a little bit of time, trying to calculate some probability for a game
* red : https://www.kaggle.com/dansbecker/random-forests
* done the kaggle exercice on random forests, but it was not very helpful. I
need to implement one from scratch. Scikit-learn is cool and all but, I want to
do it the hard way
* red my notes on decision trees

# day 5: 2019-01-07

* Today, I am tired and angry, but Basically I Do Work
* Watching again :
	* deeplearning.ai C5W1L01 : why sequence models
	* deeplearning.ai C5W1L02 : notation
	* deeplearning.ai C5W1L03 : RNN Model
* deeplearning.ai C5W1L04 : Backpropagation through time

# day 6: 2019-01-08

* not very productive today
* deeplearning.ai C5W1L05 : Different types of RNNs
* Start of C5W1L06


# day 7: 2019-01-09

* C4W1L06 : Language model and sequence generation
* C5W1L07 : Sampling novel sequences
* C5W1L08 : Vanishing gradients with RNNs
* StatQuest videos
	* Bias and Variance
	* The confusion matrix
* start implementing decision trees

# day 8: 2019-01-10

* Continue to work on decision trees, it's more difficult than I expected because
I want to deal with numerics, multiple choices, ranks, and binary data. I need
to think a little bit more, before writing code
* C5W1L09 : Gated Recurrent Unit (first 11 minutes)

# day 9: 2019-01-11

* C5W1L09 : Gated Recurrent Unit 
* productivity = 0
* i've read a few pages of the book "Deep Learning" chapter 10
* copied coursera exercises on sequence models

# day 10: 2019-01-12

* keno project : bob wants to win money playing keno, and thinks there's a pattern
so I think it can actually be a funny project to apply my freshly learned knowledge
on RNNs
  * gathering data : found a website with all the results
  * tryed to install a python package to web scrap the data, but it failed
because I messed up with my environements, and really should try to clean
everything up, but I am not ready to spend time doing that
  * I probably can do that with some CLTools, probably can just use awk, I need
to learn awk. Yeah, bbut let use a better tool for the job, xpath. I know
xpath, I don't remember the syntax, because my memory is leaking real bad, so
back to my college material. Actually this file has information on Trees, need
to read it, to help me implement the random forest. That's crazy how fast I
forget things.
I like to learn about things before using them, I think the time I spend learning
helps me being more productive and losing less time, but I am still not sure
about how much time learning is too much.
I need to stop thinking and just go in there, flying, dominating, with speed,
momentum and violence
let's just fucking use javascript, console.log and don't care about anything
Done, what is wrong with me, it's 4:20 am

# day 11: 2019-01-13

* tested the csv file I created this night
* create git repo for the Keno project
* C5W1L10 : LSTM

# day 12: 2019-01-14

* Randomly started to watch StatQuest's video on PCA (because i don't remember how
it works), and it started to talk about eigenvectors and gave intuitions about
what eigenvectors are and why we use them. I studied eigen vectors and values
at college, but I don't really know the intuition behind them, and their applications
* Looking for more information on Eigendecomposition I've stumbled on a
3BLUE1BROWN serie on Linear Algebra, and because 3BLUE1BROWN's videos are
excellent I am just going to watch the whole serie without taking notes (because
I've already learned Linear Algebra). The goal is to have a better intuition of
Linear Algebra, I think it will help me visualize things in AI, I mean neural
nets don't apply linear transformations but eeeeeeeeeeeeeee :
  * chapter 1 - vectors, what even are they ?
  * chapter 2 - linear combinations, span and basis vectors
  * chapter 3 - linear transformations and matrices
  * chapter 4 - Matrix multiplication as composition
  * chapter 5 - 3-dimensional linear transformations
  * chapter 6 - The determinant
* Actually I took notes

# day 13: 2019-01-15

* These videos are gold, 3BLUE1BROWN serie on Linear Algebra
  * chapter 7 - Inverse matrices, column space and null space
  * chapter 8 - Nonsquare matrices as transformations between dimensions
  * chapter 9 - Dot products and duality
* Read http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ up to Topology of tanh Layers

# day 14: 2019-01-16

* read my notes on the linear algebra intuition serie
* chapter 10 - cross products
* chapter 11 - Cross products in the light of linear transformations
  * that one is pretty tough, I need to watch it fully first, understand it, and
  then take notes

# day 15: 2019-01-17

* busy day, time goes flying
* chapter 12 - Change of Basis, I think I have a better intuition than 3B1B
himself, I post a comment on the video explaining my point of view.

# day 16: 2019-01-18

* chapter 13 - Eigenvectors and eigenvalues

# day 17: 2019-01-19

* chapter 14 - Abstract vector spaces
The serie on the linear algebra essence is finally over, it was great. Now I
need to learn more about topology,
* StatQuest video on PCA

# day 18: 2019-01-20

* a quick tour of google colab, I am probably going to use it to work on the
keno project
* read my notes on RNN, I need to implement one from scratch to really know how
they work, know the shapes of the matrices...
* deeplearning.ai C5W1L11 - BiDirectional RNN

# day 19: 2019-01-21

* started a course on Financial Markets for some reason, maybe I thought I add
too much free time or whatever. I am just very curious about finance and markets
and money, plus there is a lot of machine learning in this area
* C5W1L12 : Deep RNNs, finally over with the first week of course 5, I feel like
i took forever, now the assignements, 

# day 20: 2019-01-22

* Started implementing a RNN "from scratch". I was expecting the networking to
have more layers, not just `x,a-->tanh-->softmax-->y`, but `x,a-->tanh-->a-->tanh-->...`
sharing all the different `a`. The coursera assignements are really too easy,
you don't need to think to complete them, but it's still interesting to see
the different blocks to build a RNN. I see it more like a way to validate my
understanding of the course rather than a coding challenge. I mean now, there are
tensorflow/keras, pytorch to build model easily, so I don't know if I should/
need to implement one from scratch all by myself. OMG I can't stop forgetting
how to use python and numpy, that's lame.

# day 21: 2019-01-23

* I am mad, and can't focus
* Complete LSTM code, up to : Backpropagation in recurrent neural networks

# day 22: 2019-01-24

* DeepMind Startcraft II demo boys, deepmind won, TLO got rekt
* Started project Keno on Google colab
* watch some part of Getting Started with TensorFlow and Deep Learning | SciPy 2018 Tutorial | Josh Gordon

# day 23: 2019-01-25

* watch a little bit of yesterday video <https://youtu.be/tYYVSEHq-io>
GOAL : watch all the vid rapidly https://youtu.be/tYYVSEHq-io?t=1577
start implementing a RNN or LSTM or GRU with tensorflow keras keno, google colab

# day 24: 2019-01-26

* I am starting to be a little mad, I have been trying to implement a RNN using
tensorflow for the stupid keno projet for 3 days, and I am wasting a lot of time
to do something that should be simple. I thought there would be an example of
how to do it with keras on the tensorflow website, but no, they use the old
fashion data flow. Because I don't use tensorflow enough, because I didn't spend
enough time in it, I don't remember/know how it works, so I decided to learn
more about it and started looking at Josh Gordon's video, but he says "if you
see that type of code, run away", and he's like we're going to show you how to
do this amazing cool stuff and 10 seconds later start a notebook to classify
fashion-NMIST. So 2h for not much, no thank you, so let's try to dig deeper, and
find Sequence Models in keras. There is CuDNNLSTM, ok nice. But, then we can
see another problem of the tensorflow doc : it doesn't give a hint on how to use
it, how to format the input data, what's the output, how to "build the model"...
For example : 'units : positive integer, dimnesionality of the output space'.
What does that even mean ? Dimensionality of the output space, is that the number
of outputs, is that the shape of the output, is that the number of LSTM cells,
I mean wtf, can't you just write "number of output nodes". Let's watch some tuto
on how to implement LSTM with tensorflow and keras. Nice the tutorials doesn't
explain anything I don't already know, and some of them litteraly say "I don't
know" ; ok I am mad, I am just going to dive into the doc, search on stackoverflow
and do it the hard way, It's probably going to be easier and faster -.-

# day 25: 2019-01-27

* 1h a day every day, makes me do bad things, because I need to do it, and it's
very short I try to do what I need to do as fast as possible without thinking too
much, without automating things. I am very disappointed by myself, I need to
work harder, so let's start doing thing properly.
* let's create a proper dataset for Keno, by web scrapping the data and process it to
create a ready-to-use dataset
  * script scra.py to scrap the web page, get the numbers, transform them in one-hot vectors and create CSV file, it was really fucking easy, I really need to stop being lazy it makes my life harder

# day 26: 2019-01-28

* I can't find the information I am looking for and it's pissing me off, I swear no one ever tried, no one knows, it's nowhere. Ok guess I am not ready yet, I am going to start building simpler model, do a few exercices using RNNs, LSTMs, GRU, before argpiuanrpg I mean fuck !, it's supposed to be simpler, If i implemented it from scratch or using dataflow it would be already done, what a fucking waste of time
* Tried sentdex tuto, but I've got dimensions errors, FFS, I am done, I quit, and probably not going to touch that shit for 1 week

# day 27: 2019-01-29

* OMEGALUL, I am an idiot, fix sentdex tuto error, just forgot to remove a parameter, but I am getting really poor results, can't stop being stupid and not focused, <https://youtu.be/BSpXCRTOLJA> : DONE, I've learn nothing, great.
* But it's fun, let follow this one : <https://youtu.be/ne-dpRdNReI>
* Learned about python "f-strings", pandas dataframe shift method

# day 28: 2019-01-30

* busy day 7 to 22, but I ain't no bitch
* sentdex part9, looks like spagetthi code to me

# day 29: 2019-01-31

* sentdex part10
* sentdex part11

# day 30: 2019-02-01

* Python Data Science Handbook, Chapter 2
  * read again p 33 to 63
  * read p64 to 78 (Broadcasting, Comparisons, Masks and Boolean Logic)

# day 31: 2019-02-02

* preprocess the data for the keno project
* build a first model, get 70% accuracy, but I am not really sure about that chief

# day 32: 2019-02-03

* Python Data Science Handbook, Chapter 2
  * p. 78 to 88 (Fancy indexing, array sorting, partitioning)
* (sundays are the worst, I can't do shit from 5pm to 11pm)

# day 33: 2019-02-04

* Python Data Science Handbook, Chapter 2 p. 89 to 96 (example : k-Nearest Neighbors, structured data)
* Just a bit of fun:
  * How to make an amazing tensorflow chatbot easily <https://youtu.be/SJDEOWLHYVo>, omeglul, i forgot that Siraj's videos don't teach anything
  * [TensorFlow Examples](https://github.com/aymericdamien/TensorFlow-Examples)
    * Introduction (hello world, basic operations, tensorflow eager api basics)
    * Linear Regression with Eager API
* **thoughts**: I kinda want 2 learn more about the TensorFlow low level API,
because what I really like about ML and DL is the math, and keras is useful and
fast but I don't want to do that, I don't want to be a script kiddy

# day 34: 2019-02-05

* **Thoughts** I am wasting a lot of time in my day, and could do more, because :
  * I am really tired, and out of shape, my brain is lacking oxigen (it hurts a little bit), I need 2 naps a day to survive
  * 8hours of lecture is boring and not very effective (but college)
  * I am not focused enough
  * I want to work on a bigger project, probably going to look what's up on kaggle tomorrow
* Linear Regression Example (without eager exec)
  * <https://github.com/aymericdamien/TensorFlow-Examples/blob/master/notebooks/2_BasicModels/linear_regression.ipynb>
  * <https://www.tensorflow.org/guide/low_level_intro>

# day 35: 2019-02-06

* I am about to have a contract to finish the work on a scheduler, it's a 
constrained combinatorial optimization problem, and a genetic algorithm is used
(good ol' fashion "ai")
* Constraint satisfaction problems <https://youtu.be/lCrHYT_EhDs>
* alldiff <https://www.math.unipd.it/~frossi/alldiff.pdf>

# day 36: 2019-02-07

* **thoughts** I am so focused on DL that I almost forgot what AI is all about,
so I rewatch MIT lectures to go back to the main ideas, and start 6.0002 because
they teach concept I need to learn (i.e. Monte Carlo Simulation). It also helps
me to remember names of algo (when you have a name for an idea you got power
over it)
* rewatch MIT 6.034
  * lec1 intro and scope <https://youtu.be/TjZBTDzGeGg>
  * lec2 Reasoning: Goal Trees and Problem solving
* genetics algs (francisco Lacobelli) <https://youtu.be/qt85_CinKwo>
* MIT 6.0002 Introduction to Computational Thinking and Data Science
  * lec 1 Introduction and Optimization Problems <https://youtu.be/C1lhuz6pZC0>

# day 37: 2019-02-08

* MIT 6.034 lec3 Reasoning, Goal trees and rule-based expert system

# day 38: 2019-02-09

* MIT 6.0002 lec2 Optimization problems
  * search tree
  * dynamic programming
  * exercises

# day 39: 2019-02-10

* MIT 6.0002 lec3. Graph-theoretic Models

# day 40: 2019-02-11

* **thoughts** the reason why I am watching MIT 6.0002 lectures is because it
helps me remember the basic search/optimization algos and gives implementation
of them, I want to reach at least lec6 to see Monte Carlo Simulation. 
* Implement Node, Edge, Digraph, Graph, and algos in ldl/notDL/graph
  from MIT 6.0002 lec3. Graph-theoretic Models
* MIT 6.0002 lec4. Stochastic Thinking

# day 41: 2019-02-12

* MIT 6.0002 lec5. Random Walk

# day 42: 2019-02-13

* **thoughts** I should probably not have spent that much time on these lectures
because they are pretty basic, but it was a nice break, I enjoy MIT lectures,
but maybe not the best use of my time
* MIT 6.0002 lec6. Monte Carlo Simulation

# day 43: 2019-02-14

* Analyzing the code of the Baker project

# day 44: 2019-02-16

* **thoughts** Fuck, I missed a day, not because I was too busy, but because I
couldn't focus, because I have an addiction. I am addicted to distraction. It
used to be facebook, then instagram, then youtube, then video games, then
youtube again, now it's video games and twitch. I started to prevent myself from
overthinking, a few years ago I couldn't stop thinking, I struggled to fall
asleep, I sometimes struggled to listen to people because of my own unstoppable
thoughts. Distractions helped me, but now it's almost like I am not thinking at
all, I am preventing myself from doing anything.
* Analyzing the code of the Baker project, to try to answer the questions :
  * What was done ?
  * What needs to be done ?
* I am a little bit scared, because it's super slow and it doesn't seem to work
properly, it's not solving the problem


# day 45: 2019-02-17

* A little bit of baker
* read my notes on MIT 6.034
* beginning of MIT 6.034 lec 7. Constraints: Interpreting Line Drawings

# day 46: 2019-02-18

* **thoughts** Fuck, busy fucking day, and tomorrow will probably be worse
* a lil bit of time on baker

# day 47: 2019-02-19

* **Thoughts** Technically I spend about 20 minutes doing ML related stuff, but
on the 19 hours I was awake I probably waste 1hour in total. So it's a Le FAIL,
I need to be more careful

# day 48: 2019-02-20

* Getting intel for the Baker project, and oh it's not looking good for me,
trying to answer the questions "can I make it work ? or do i need to restart
everything from scratch ?"

# day 49: 2019-02-22

* **thoughts** If I am not careful, if I am not focusing, time flies
* baker project

# day 50: 2019-02-23

* **thought** i hate myself, I started working at 3pm and realize I was thinking
at half my speed, so I tried to play a mine game as fast as I can, but I was so
slow I need to move, walk or whatever, but it didn't work, and I ended up
somehow playing chess the whole day, and because I am a terrible player I played
small exercises. I've started to recognize a few patterns
* baker project

# day 50: 2019-02-24

* Baker project, when you code don't be a ninja <https://javascript.info/ninja-code?fbclid=IwAR0Aa6yNYngRTuHD-vG9rmaNwM7Ma7deCwmxZtW0zvgJRrEU9U1MCChm6W4>

# day 51: 2019-02-25

* **thoughts** the baker project is wasting my time, so I'll do something useful
instead today, let's start week 2 of Course 5 of deeplearning.ai on NLP and word
embeddings
* Deeplearning.ai C5W2L01 Word Representation
* Deeplearning.ai C5W2L02 Using Word Embeddings
* Deeplearning.ai C5W2L03 Properties of Word Embeddings

# day 52: 2019-02-26

* **thoughts** I mad because I waste about 2h30 because of things/people I am not
in control, I had no control on these situations, can't do anything about it,
not trying to make excuses (a little bit, but life is hard)
* Just a bit to prepare myself to thursday google hash code

# day 53: 2019-02-27

* NOTES TO MYSELF:
  * Guide to learn DeepRL by OpenAI : <http://spinningup.openai.com/en/latest/user/introduction.html>
  * and a lecture : <https://youtu.be/fdY7dt3ijgY>
* Deeplearning.ai C5W2L04 : Embedding matrix
* Deeplearning.ai C5W2L05 : Learning word embeddings
* lil bit of thought process on the pizza hash code problem (just for fun, but
don't have time)


# day 54: 2019-02-28

* Google Hash code, it was fun, our team was not organized and didn't really
care, I spent 1h on a bug, and i've not figured it out yet, really fucking weird,
but it was fun
* I found the error, but I am not getting better results with the genetic algo
I've started building than with random pick, and now I am mad

# day 55: 2019-03-01

* I want to make a genetic algorithm that work, because It should work right ?
restart from scratch
