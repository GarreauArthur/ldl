32

Goal : work at least an hour a day on AI/ML/DL/Coding/Project.
My goals in AI are :

* learn more models (RNN, GAN, RL, Trees)
* gain more practical experience
* win at least one kaggle competition

# days -2: 2019-01-01

* Watch last videos of deeplearning.ai course 2

# day -1: 2019-01-02

* Getting use to kaggle and learning tensorflow
* Titanic: Machine Learning from Disaster
* <https://www.kaggle.com/cqncpdp/one-sinking-boy>

# day 1: 2019-01-03

* Pledging
* starting this log
* Python Data Science Handbook pages 47 to 58
* bash script (nl.sh) to easily add new entries to this log

# day 2: 2019-01-04

* fix small bug of bash script
* Python Data Science Handbook pages 59 to 62 (lol, i feel sick)

Because Random forest seems to be commonly used in kaggle competition, and

* Learning about and Taking notes on Decision trees : 
  * part 1: <https://youtu.be/7VeUPuFGJHk>
  * part 2: <https://youtu.be/wpNl-JwwplA>
* Learning about Random forest : part 1 <https://youtu.be/J4Wdy0Wc_xQ>

# day 3: 2019-01-05

* Learning about Random forest :
  * end of part 1 <https://youtu.be/J4Wdy0Wc_xQ>
  * part 2 <https://youtu.be/nyxTdL_4Q-Q>

# day 4: 2019-01-06

* i've waste a little bit of time, trying to calculate some probability for a game
* red : https://www.kaggle.com/dansbecker/random-forests
* done the kaggle exercice on random forests, but it was not very helpful. I
need to implement one from scratch. Scikit-learn is cool and all but, I want to
do it the hard way
* red my notes on decision trees

# day 5: 2019-01-07

* Today, I am tired and angry, but Basically I Do Work
* Watching again :
	* deeplearning.ai C5W1L01 : why sequence models
	* deeplearning.ai C5W1L02 : notation
	* deeplearning.ai C5W1L03 : RNN Model
* deeplearning.ai C5W1L04 : Backpropagation through time

# day 6: 2019-01-08

* not very productive today
* deeplearning.ai C5W1L05 : Different types of RNNs
* Start of C5W1L06


# day 7: 2019-01-09

* C4W1L06 : Language model and sequence generation
* C5W1L07 : Sampling novel sequences
* C5W1L08 : Vanishing gradients with RNNs
* StatQuest videos
	* Bias and Variance
	* The confusion matrix
* start implementing decision trees

# day 8: 2019-01-10

* Continue to work on decision trees, it's more difficult than I expected because
I want to deal with numerics, multiple choices, ranks, and binary data. I need
to think a little bit more, before writing code
* C5W1L09 : Gated Recurrent Unit (first 11 minutes)

# day 9: 2019-01-11

* C5W1L09 : Gated Recurrent Unit 
* productivity = 0
* i've read a few pages of the book "Deep Learning" chapter 10
* copied coursera exercises on sequence models

# day 10: 2019-01-12

* keno project : bob wants to win money playing keno, and thinks there's a pattern
so I think it can actually be a funny project to apply my freshly learned knowledge
on RNNs
  * gathering data : found a website with all the results
  * tryed to install a python package to web scrap the data, but it failed
because I messed up with my environements, and really should try to clean
everything up, but I am not ready to spend time doing that
  * I probably can do that with some CLTools, probably can just use awk, I need
to learn awk. Yeah, bbut let use a better tool for the job, xpath. I know
xpath, I don't remember the syntax, because my memory is leaking real bad, so
back to my college material. Actually this file has information on Trees, need
to read it, to help me implement the random forest. That's crazy how fast I
forget things.
I like to learn about things before using them, I think the time I spend learning
helps me being more productive and losing less time, but I am still not sure
about how much time learning is too much.
I need to stop thinking and just go in there, flying, dominating, with speed,
momentum and violence
let's just fucking use javascript, console.log and don't care about anything
Done, what is wrong with me, it's 4:20 am

# day 11: 2019-01-13

* tested the csv file I created this night
* create git repo for the Keno project
* C5W1L10 : LSTM

# day 12: 2019-01-14

* Randomly started to watch StatQuest's video on PCA (because i don't remember how
it works), and it started to talk about eigenvectors and gave intuitions about
what eigenvectors are and why we use them. I studied eigen vectors and values
at college, but I don't really know the intuition behind them, and their applications
* Looking for more information on Eigendecomposition I've stumbled on a
3BLUE1BROWN serie on Linear Algebra, and because 3BLUE1BROWN's videos are
excellent I am just going to watch the whole serie without taking notes (because
I've already learned Linear Algebra). The goal is to have a better intuition of
Linear Algebra, I think it will help me visualize things in AI, I mean neural
nets don't apply linear transformations but eeeeeeeeeeeeeee :
  * chapter 1 - vectors, what even are they ?
  * chapter 2 - linear combinations, span and basis vectors
  * chapter 3 - linear transformations and matrices
  * chapter 4 - Matrix multiplication as composition
  * chapter 5 - 3-dimensional linear transformations
  * chapter 6 - The determinant
* Actually I took notes

# day 13: 2019-01-15

* These videos are gold, 3BLUE1BROWN serie on Linear Algebra
  * chapter 7 - Inverse matrices, column space and null space
  * chapter 8 - Nonsquare matrices as transformations between dimensions
  * chapter 9 - Dot products and duality
* Read http://colah.github.io/posts/2014-03-NN-Manifolds-Topology/ up to Topology of tanh Layers

# day 14: 2019-01-16

* read my notes on the linear algebra intuition serie
* chapter 10 - cross products
* chapter 11 - Cross products in the light of linear transformations
  * that one is pretty tough, I need to watch it fully first, understand it, and
  then take notes

# day 15: 2019-01-17

* busy day, time goes flying
* chapter 12 - Change of Basis, I think I have a better intuition than 3B1B
himself, I post a comment on the video explaining my point of view.

# day 16: 2019-01-18

* chapter 13 - Eigenvectors and eigenvalues

# day 17: 2019-01-19

* chapter 14 - Abstract vector spaces
The serie on the linear algebra essence is finally over, it was great. Now I
need to learn more about topology,
* StatQuest video on PCA

# day 18: 2019-01-20

* a quick tour of google colab, I am probably going to use it to work on the
keno project
* read my notes on RNN, I need to implement one from scratch to really know how
they work, know the shapes of the matrices...
* deeplearning.ai C5W1L11 - BiDirectional RNN

# day 19: 2019-01-21

* started a course on Financial Markets for some reason, maybe I thought I add
too much free time or whatever. I am just very curious about finance and markets
and money, plus there is a lot of machine learning in this area
* C5W1L12 : Deep RNNs, finally over with the first week of course 5, I feel like
i took forever, now the assignements, 

# day 20: 2019-01-22

* Started implementing a RNN "from scratch". I was expecting the networking to
have more layers, not just `x,a-->tanh-->softmax-->y`, but `x,a-->tanh-->a-->tanh-->...`
sharing all the different `a`. The coursera assignements are really too easy,
you don't need to think to complete them, but it's still interesting to see
the different blocks to build a RNN. I see it more like a way to validate my
understanding of the course rather than a coding challenge. I mean now, there are
tensorflow/keras, pytorch to build model easily, so I don't know if I should/
need to implement one from scratch all by myself. OMG I can't stop forgetting
how to use python and numpy, that's lame.

# day 21: 2019-01-23

* I am mad, and can't focus
* Complete LSTM code, up to : Backpropagation in recurrent neural networks

# day 22: 2019-01-24

* DeepMind Startcraft II demo boys, deepmind won, TLO got rekt
* Started project Keno on Google colab
* watch some part of Getting Started with TensorFlow and Deep Learning | SciPy 2018 Tutorial | Josh Gordon

# day 23: 2019-01-25

* watch a little bit of yesterday video <https://youtu.be/tYYVSEHq-io>
GOAL : watch all the vid rapidly https://youtu.be/tYYVSEHq-io?t=1577
start implementing a RNN or LSTM or GRU with tensorflow keras keno, google colab

# day 24: 2019-01-26

* I am starting to be a little mad, I have been trying to implement a RNN using
tensorflow for the stupid keno projet for 3 days, and I am wasting a lot of time
to do something that should be simple. I thought there would be an example of
how to do it with keras on the tensorflow website, but no, they use the old
fashion data flow. Because I don't use tensorflow enough, because I didn't spend
enough time in it, I don't remember/know how it works, so I decided to learn
more about it and started looking at Josh Gordon's video, but he says "if you
see that type of code, run away", and he's like we're going to show you how to
do this amazing cool stuff and 10 seconds later start a notebook to classify
fashion-NMIST. So 2h for not much, no thank you, so let's try to dig deeper, and
find Sequence Models in keras. There is CuDNNLSTM, ok nice. But, then we can
see another problem of the tensorflow doc : it doesn't give a hint on how to use
it, how to format the input data, what's the output, how to "build the model"...
For example : 'units : positive integer, dimnesionality of the output space'.
What does that even mean ? Dimensionality of the output space, is that the number
of outputs, is that the shape of the output, is that the number of LSTM cells,
I mean wtf, can't you just write "number of output nodes". Let's watch some tuto
on how to implement LSTM with tensorflow and keras. Nice the tutorials doesn't
explain anything I don't already know, and some of them litteraly say "I don't
know" ; ok I am mad, I am just going to dive into the doc, search on stackoverflow
and do it the hard way, It's probably going to be easier and faster -.-

# day 25: 2019-01-27

* 1h a day every day, makes me do bad things, because I need to do it, and it's
very short I try to do what I need to do as fast as possible without thinking too
much, without automating things. I am very disappointed by myself, I need to
work harder, so let's start doing thing properly.
* let's create a proper dataset for Keno, by web scrapping the data and process it to
create a ready-to-use dataset
  * script scra.py to scrap the web page, get the numbers, transform them in one-hot vectors and create CSV file, it was really fucking easy, I really need to stop being lazy it makes my life harder

# day 26: 2019-01-28

* I can't find the information I am looking for and it's pissing me off, I swear no one ever tried, no one knows, it's nowhere. Ok guess I am not ready yet, I am going to start building simpler model, do a few exercices using RNNs, LSTMs, GRU, before argpiuanrpg I mean fuck !, it's supposed to be simpler, If i implemented it from scratch or using dataflow it would be already done, what a fucking waste of time
* Tried sentdex tuto, but I've got dimensions errors, FFS, I am done, I quit, and probably not going to touch that shit for 1 week

# day 27: 2019-01-29

* OMEGALUL, I am an idiot, fix sentdex tuto error, just forgot to remove a parameter, but I am getting really poor results, can't stop being stupid and not focused, <https://youtu.be/BSpXCRTOLJA> : DONE, I've learn nothing, great.
* But it's fun, let follow this one : <https://youtu.be/ne-dpRdNReI>
* Learned about python "f-strings", pandas dataframe shift method

# day 28: 2019-01-30

* busy day 7 to 22, but I ain't no bitch
* sentdex part9, looks like spagetthi code to me

# day 29: 2019-01-31

* sentdex part10
* sentdex part11

# day 30: 2019-02-01

* Python Data Science Handbook, Chapter 2
  * read again p 33 to 63
  * read p64 to 78 (Broadcasting, Comparisons, Masks and Boolean Logic)

# day 31: 2019-02-02

* preprocess the data for the keno project
* build a first model, get 70% accuracy, but I am not really sure about that chief

# day 32: 2019-02-03

* Python Data Science Handbook, Chapter 2
  * p. 78 to 88 (Fancy indexing, array sorting, partitioning)
* (sundays are the worst, I can't do shit from 5pm to 11pm)
