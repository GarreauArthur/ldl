{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Let's build a Deep NN from scratch\n",
    "\n",
    "Greatly inspired by deeplearning.ai assignement\n",
    "\n",
    "**Notation**:\n",
    "- Superscript $[l]$ denotes a quantity associated with the $l^{th}$ layer. \n",
    "    - Example: $a^{[L]}$ is the $L^{th}$ layer activation. $W^{[L]}$ and $b^{[L]}$ are the $L^{th}$ layer parameters.\n",
    "- Superscript $(i)$ denotes a quantity associated with the $i^{th}$ example. \n",
    "    - Example: $x^{(i)}$ is the $i^{th}$ training example.\n",
    "- Lowerscript $i$ denotes the $i^{th}$ entry of a vector.\n",
    "    - Example: $a^{[l]}_i$ denotes the $i^{th}$ entry of the $l^{th}$ layer's activations).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Packages\n",
    "\n",
    "To build our DNN in python we will use a few useful packages\n",
    "\n",
    "* **numpy** for mathematical operations\n",
    "* **matplotlib** to visualize things"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plot\n",
    "from testCases_v2 import * # for testing purpose"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialization\n",
    "\n",
    "The initialization step is extremely important, there are a lot of different techniques to do it, but the main idea is to use small random numbers to keep w near 0 (to avoid overfitting, and symmetry problems). However, we can initialize b to 0.\n",
    "\n",
    "We will store the number of units in each layer in an array called `layer_dims`. The shape of the weight matrix is `(layer_dims[l], layer_dims[l-1])` and the shape of the bias vector is `(layer_dims[l], 1)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def initialize_parameters_deep(layer_dims):\n",
    "    \"\"\"\n",
    "    @param - layer_dims - python array (list) containing the dimensions of each layer in our network\n",
    "    @return parameters - python dictionary containing the initialized parameters \"W1\", \"b1\", ..., \"WL\", \"bL\":\n",
    "                    Wl - weight matrix of shape (layer_dims[l], layer_dims[l-1])\n",
    "                    bl - bias vector of shape (layer_dims[l], 1)\n",
    "    \"\"\"\n",
    "    parameters = {}\n",
    "    L = len(layer_dims)            # number of layers in the network\n",
    "\n",
    "    for l in range(1, L):\n",
    "        parameters['W' + str(l)] = np.random.randn(layer_dims[l],layer_dims[l-1])* 0.01\n",
    "        parameters['b' + str(l)] = np.zeros((layer_dims[l],1))\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Example of use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.01444114 -0.00504466  0.00160037]\n",
      " [ 0.00876169  0.00315635 -0.02022201]\n",
      " [-0.00306204  0.00827975  0.00230095]\n",
      " [ 0.00762011 -0.00222328 -0.00200758]\n",
      " [ 0.00186561  0.00410052  0.001983  ]]\n",
      "b1 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n",
      "W2 = [[ 0.00119009 -0.00670662  0.00377564  0.00121821  0.01129484]\n",
      " [ 0.01198918  0.00185156 -0.00375285 -0.0063873   0.00423494]\n",
      " [ 0.0007734  -0.00343854  0.00043597 -0.00620001  0.00698032]\n",
      " [-0.00447129  0.01224508  0.00403492  0.00593579 -0.01094912]]\n",
      "b2 = [[0.]\n",
      " [0.]\n",
      " [0.]\n",
      " [0.]]\n"
     ]
    }
   ],
   "source": [
    "parameters = initialize_parameters_deep([3,5,4,2])\n",
    "print(\"W1 = \" + str(parameters[\"W1\"]))\n",
    "print(\"b1 = \" + str(parameters[\"b1\"]))\n",
    "print(\"W2 = \" + str(parameters[\"W2\"]))\n",
    "print(\"b2 = \" + str(parameters[\"b2\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Forward propagation\n",
    "\n",
    "The forward propagation, propagate the data through the network, each neuron has 2 elements:\n",
    "\n",
    "1. A linear forward module\n",
    "2. Activation function\n",
    "\n",
    "\n",
    "Forward propagation gives us $\\forall l \\in \\left \\{ 1, 2,\\cdots , L \\right \\}, z^{[l]}\\text{ and }a^{[l]}$\n",
    "\n",
    "### Linear forward module\n",
    "\n",
    "The linear forward module (vectorized over all the examples) computes the following equation:\n",
    "\n",
    "$$Z^{[l]} = W^{[l]}A^{[l-1]} +b^{[l]}\\tag{linear}$$\n",
    "\n",
    "where $A^{[0]} = X$ \n",
    "\n",
    "and\n",
    "\n",
    "$ W = \\begin{bmatrix}\n",
    "\\leftarrow & w_0^{[l]^T} &\\rightarrow \\\\\n",
    "\\leftarrow & ... & \\rightarrow  \\\\ \n",
    "\\leftarrow & w_i^{[l]^T} & \\rightarrow  \\\\\n",
    "\\leftarrow & ... & \\rightarrow  \\\\\n",
    "\\leftarrow & w_{n^{[l]}}^{[l]^T} & \\rightarrow  \n",
    "\\end{bmatrix}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_forward(A,W,b):\n",
    "    \"\"\"\n",
    "    Linear part of a layer's forward propagation\n",
    "    \n",
    "    @param  A - activation from the previous layer (size of previous layer, number of examples)\n",
    "    @param  W - weights matrix (size of current layer)\n",
    "    @param  b - bias vector, (size of current layer)\n",
    "    @return Z - input of the activation function (pre-activation parameter)\n",
    "    @return cache - python tuple containing A, W and b, useful for computing back prop\n",
    "    \"\"\"\n",
    "    Z = np.dot(W,A)+b\n",
    "    cache = (A, W, b)\n",
    "    return Z, cache"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Activation function\n",
    "\n",
    "Each neuron has an activation function,  there are several possibilities : Sigmoid, ReLU, tanh, leaky ReLU ... Here, we are going to use ReLU and sigmoid as activation functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(Z):\n",
    "    \"\"\"\n",
    "    @param  Z     - numpy array of any shape\n",
    "    @return A     - output of sigmoid(Z), same shape as Z\n",
    "    @return cache - return Z, useful during backpropagation\n",
    "    \"\"\"\n",
    "    A = 1/(1+np.exp(-Z))\n",
    "    cache = Z\n",
    "    return A, cache\n",
    "\n",
    "def relu(Z):\n",
    "    \"\"\"\n",
    "    @param Z - output of the linear layer, any shape\n",
    "    @return A - ReLU(Z), same shape as Z\n",
    "    @return cache - return Z, useful during backprop\n",
    "    \"\"\"\n",
    "    A = np.maximum(0,Z)\n",
    "    cache = Z\n",
    "    assert(A.shape == Z.shape)\n",
    "    return A, cache\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For more convenience, we are going to group all these functions (linear and activation) into one function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_activation_forward(A_prev, W, b, activation):\n",
    "    \"\"\"\n",
    "    Forward propagation\n",
    "    \n",
    "    @param A_prev     - Activations from previous layer (size of prev layer, number of examples)\n",
    "    @param W          - Weights matrix (size of current layer, size of previous layer)\n",
    "    @param b          - Bias vector (size of current layer, 1)\n",
    "    @param activation - String representing the activation function to use\n",
    "    @return A         - The result, post-activation value\n",
    "    @return cache     - linear_cache and activation_cache\n",
    "    \"\"\"\n",
    "    Z, linear_cache = linear_forward(A_prev, W, b)\n",
    "    if activation == \"sigmoid\":\n",
    "        A, activation_cache = sigmoid(Z)\n",
    "    elif activation == \"relu\":\n",
    "        A, activation_cache = relu(Z)\n",
    "        \n",
    "    cache = (linear_cache, activation_cache)\n",
    "    return A, cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "With sigmoid: A = [[0.96890023 0.11013289]]\n",
      "With ReLU: A = [[3.43896131 0.        ]]\n"
     ]
    }
   ],
   "source": [
    "A_prev, W, b = linear_activation_forward_test_case()\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"sigmoid\")\n",
    "print(\"With sigmoid: A = \" + str(A))\n",
    "\n",
    "A, linear_activation_cache = linear_activation_forward(A_prev, W, b, activation = \"relu\")\n",
    "print(\"With ReLU: A = \" + str(A))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Our model\n",
    "\n",
    "For even more convenience when implementing an L-layer NN, we will need a function that replicates the previous one `linear_activation_forward` L times. Here, we define the model of our DNN.\n",
    "\n",
    "In this example, we are going to use L-1 layers composed of ReLUnits, and the last layer will use one sigmoid unit.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def L_model_forward(X, parameters):\n",
    "    \"\"\"\n",
    "    Forward propagation for the [LINEAR->RELU]*(L-1)->LINEAR->SIGMOID computation\n",
    "    \n",
    "    @param X          - input data (input size, number examples)\n",
    "    @param parameters - output of initalize_parameters_deep()\n",
    "    @return AL        - last post-activation (L-1 firsts)\n",
    "    @return caches    - caches containing every cache of linear_relu_forward() and linear_sigmoid_forward (last one)\n",
    "    \n",
    "    \"\"\"\n",
    "    caches = []\n",
    "    A = X\n",
    "    L = len(parameters)//2 # number of layers in the nn\n",
    "    \n",
    "    # Implement [LINEAR -> RELU]*(L-1). Add \"cache\" to the \"caches\"\n",
    "    for l in range(1,L):\n",
    "        A_prev = A\n",
    "        A, cache = linear_activation_forward(A_prev, parameters[\"W\"+str(l)], parameters[\"b\"+str(l)], \"relu\")\n",
    "        caches.append(cache)\n",
    "    # Implement LINEAR -> SIGMOID. Add \"cache\" to the \"caches\"\n",
    "    AL, cache = linear_activation_forward(A, parameters[\"W\"+str(L)], parameters[\"b\"+str(L)], \"sigmoid\")\n",
    "    caches.append(cache)\n",
    "    \n",
    "    return AL, caches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AL = [[0.17007265 0.2524272 ]]\n",
      "Length of caches list = 2\n"
     ]
    }
   ],
   "source": [
    "X, parameters = L_model_forward_test_case()\n",
    "AL, caches = L_model_forward(X, parameters)\n",
    "print(\"AL = \" + str(AL))\n",
    "print(\"Length of caches list = \" + str(len(caches)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That was easy, we now have a full forward propagation that takes the input X and outputs a row vector $A^{[L]}$ containing our predictions. It also records all intermediate values in \"caches\". Using $A^{[L]}$, we can compute the cost of our predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost function\n",
    "\n",
    "It's time to measure how well our NN performed. We will use the cross-entropy cost J to do that :\n",
    "\n",
    "$$J = -\\frac{1}{m} \\sum\\limits_{i = 1}^{m} (y^{(i)}\\log\\left(a^{[L] (i)}\\right) + (1-y^{(i)})\\log\\left(1- a^{[L](i)}\\right))$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cost(AL, Y):\n",
    "    \"\"\"\n",
    "    Compute the cost function\n",
    "    @param AL    - probability vector corresponding to our predictions (1, number of examples)\n",
    "    @param Y     - True label vector (1, number of examples)\n",
    "    @return cost - cross-entropy cost\n",
    "    \"\"\"\n",
    "    m = Y.shape[1]\n",
    "    cost = -1*np.sum(Y*np.log(AL)+(1-Y)*np.log(1-AL))/m\n",
    "    cost = np.squeeze(cost) # to make sure cost's shape is what we expect\n",
    "    return cost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember: `*` is an element wise multiplication (we also can use `np.multiply()`, while `np.dot()` is the dot product (matrix product)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Backpropagation\n",
    "\n",
    "The idea of backpropagation is to compute the derivates of the cost function with respect to w, to be able to update the values of\n",
    "all w. In order to do that, we use an interesting property called \"Chain rule\".\n",
    "\n",
    "![The best chain rule diagram ever made](chainrule.png)\n",
    "\n",
    "We can compute $\\frac{\\partial J}{\\partial w}$ by chaining (multiplying) the local partial derivatives. Example, imagine J is compute just after $a^{[l]}$:\n",
    "\n",
    "\n",
    "$$\\frac{\\partial J}{\\partial w^{[l-1]}} = \\frac{\\partial J}{\\partial a^{[l]}} \\cdot \\frac{\\partial a^{[l]}}{\\partial z} \\cdot \\frac{\\partial z^{[l]}}{\\partial a^{[l-1]}} \\cdot \\frac{\\partial a^{[l-1]}}{\\partial z^{[l-1]}} \\cdot \\frac{\\partial z^{[l-1]}}{\\partial w^{[l-1]}} $$\n",
    "\n",
    "To simplify this expression we introduce the following notation:\n",
    "\n",
    "$dA^{[l]} = \\frac{\\partial J}{\\partial a^{[L]}}\\cdot\\frac{\\partial a^{[L]}}{\\partial z^{[L]}} \\cdot\\cdot\\cdot \\frac{\\partial z^{[l+1]}}{\\partial a^{[l]}}$\n",
    "\n",
    "$dZ^{[l]} = dA \\cdot \\frac{\\partial a^{[l]}}{\\partial z^{[l]}}$\n",
    "\n",
    "$dw^{[l]} = dZ^{[l]} \\cdot \\frac{\\partial z^{[l]}}{\\partial w^{[l]}}$\n",
    "\n",
    "$db^{[l]} = dZ^{[l]} \\cdot \\frac{\\partial z^{[l]}}{\\partial b^{[l]}}$\n",
    "\n",
    "If we do a little bit of calculus, we get the following expressions:\n",
    "\n",
    "$dw^{[l]} = dZ^{[l]} \\cdot a^{[l-1]}$\n",
    "\n",
    "$db^{[l]} = dZ^{[l]}$\n",
    "\n",
    "$dA^{[l-1]} = w^{[l]^T}\\cdot dZ^{[l]}$\n",
    "\n",
    "Our job now is to compute these values.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear backward\n",
    "\n",
    "For layer l, the linear part is $Z^{[l]} = W^{[l]} A^{[l-1]} + b^{[l]}$\n",
    "\n",
    "Suppose we have already calculated the derivative $dZ^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial Z^{[l]}}$. We can easily get $(dW^{[l]}, db^{[l]} dA^{[l-1]})$, using the vectorized version :\n",
    "\n",
    "$$ dW^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial W^{[l]}} = \\frac{1}{m} dZ^{[l]} A^{[l-1] T}$$\n",
    "$$ db^{[l]} = \\frac{\\partial \\mathcal{L} }{\\partial b^{[l]}} = \\frac{1}{m} \\sum_{i = 1}^{m} dZ^{[l](i)}$$\n",
    "$$ dA^{[l-1]} = \\frac{\\partial \\mathcal{L} }{\\partial A^{[l-1]}} = W^{[l] T} dZ^{[l]}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_backward(dZ, cache):\n",
    "    \"\"\"\n",
    "    Implement the linear portion of backward propagation for a single layer (layer l)\n",
    "\n",
    "    Arguments:\n",
    "    @param dZ    - Gradient of the cost with respect to the linear output (of current layer l)\n",
    "    @param cache - tuple of values (A_prev, W, b) coming from the forward propagation in the current layer\n",
    "\n",
    "    \n",
    "    @return dA_prev - Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    @return dW      - Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    @return db      - Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    A_prev, W, b = cache\n",
    "    m = A_prev.shape[1]\n",
    "\n",
    "    dA_prev = np.dot(W.T,dZ)\n",
    "    dW = np.dot(dZ, A_prev.T)/m\n",
    "    db = np.sum(dZ,axis=1, keepdims=True)/m\n",
    "\n",
    "    assert (dA_prev.shape == A_prev.shape)\n",
    "    assert (dW.shape == W.shape)\n",
    "    assert (db.shape == b.shape)\n",
    "    \n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dA_prev = [[ 0.51822968 -0.19517421]\n",
      " [-0.40506361  0.15255393]\n",
      " [ 2.37496825 -0.89445391]]\n",
      "dW = [[-0.10076895  1.40685096  1.64992505]]\n",
      "db = [[0.50629448]]\n"
     ]
    }
   ],
   "source": [
    "# Set up some test inputs\n",
    "dZ, linear_cache = linear_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear-Activation backward\n",
    "\n",
    "To help implement linear_activation_backward, we provide two backward functions:\n",
    "\n",
    "* `sigmoid_backward`\n",
    "* `relu_backward`\n",
    "\n",
    "to compute $dZ$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for a single SIGMOID unit.\n",
    "\n",
    "    @param  dA    - post-activation gradient, of any shape\n",
    "    @param  cache - Z stored for computing backward propagation efficiently\n",
    "    @return dZ    - Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    \n",
    "    Z = cache    \n",
    "    s = 1/(1+np.exp(-Z))\n",
    "    dZ = dA * s * (1-s)\n",
    "    \n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def relu_backward(dA, cache):\n",
    "    \"\"\"\n",
    "    Implement backprop for a single ReLU unit\n",
    "    \n",
    "    @param dA    - post-activation gradient, of any shape (previous layer)\n",
    "    @param cache - Z computed during forward propagation\n",
    "    @return dZ   - Gradient of the cost with respect to Z\n",
    "    \"\"\"\n",
    "    Z = cache\n",
    "    dZ = np.array(dA, copy=True)# just converting dz to a correct object\n",
    "    # if Z <= 0 then dZ = 0 else dZ is just the same (=dA)\n",
    "    dZ[Z <= 0] = 0\n",
    "    return dZ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the backward propagation for one layer, from $dA^{[l]}$ it will output $dA^{[l-1]}, dW^{[l]}$ and $db^{[l]}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def linear_activation_backward(dA, cache, activation):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the LINEAR->ACTIVATION layer.\n",
    "    \n",
    "    @param dA         - post-activation gradient for current layer l \n",
    "    @param cache      - (linear_cache, activation_cache) used for computing backward propagation efficiently\n",
    "    @param activation - the activation to be used in this layer, String: \"sigmoid\" or \"relu\"\n",
    "    \n",
    "    @return dA_prev   - Gradient of the cost with respect to the activation (of the previous layer l-1), same shape as A_prev\n",
    "    @return dW        - Gradient of the cost with respect to W (current layer l), same shape as W\n",
    "    @return db        - Gradient of the cost with respect to b (current layer l), same shape as b\n",
    "    \"\"\"\n",
    "    linear_cache, activation_cache = cache\n",
    "    \n",
    "    if activation == \"relu\":\n",
    "        dZ = relu_backward(dA, activation_cache)\n",
    "    elif activation == \"sigmoid\":\n",
    "        dZ = sigmoid_backward(dA, activation_cache)\n",
    "        \n",
    "    dA_prev, dW, db = linear_backward(dZ, linear_cache)\n",
    "    return dA_prev, dW, db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigmoid:\n",
      "dA_prev = [[ 0.11017994  0.01105339]\n",
      " [ 0.09466817  0.00949723]\n",
      " [-0.05743092 -0.00576154]]\n",
      "dW = [[ 0.10266786  0.09778551 -0.01968084]]\n",
      "db = [[-0.05729622]]\n",
      "\n",
      "relu:\n",
      "dA_prev = [[ 0.44090989 -0.        ]\n",
      " [ 0.37883606 -0.        ]\n",
      " [-0.2298228   0.        ]]\n",
      "dW = [[ 0.44513824  0.37371418 -0.10478989]]\n",
      "db = [[-0.20837892]]\n"
     ]
    }
   ],
   "source": [
    "AL, linear_activation_cache = linear_activation_backward_test_case()\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"sigmoid\")\n",
    "print (\"sigmoid:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db) + \"\\n\")\n",
    "\n",
    "dA_prev, dW, db = linear_activation_backward(AL, linear_activation_cache, activation = \"relu\")\n",
    "print (\"relu:\")\n",
    "print (\"dA_prev = \"+ str(dA_prev))\n",
    "print (\"dW = \" + str(dW))\n",
    "print (\"db = \" + str(db))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### L-Model backward prop\n",
    "\n",
    "Now we are done with one layer, we can repeat these operations to implement our model. Recall that when we implemented the `L_model_forward` function, at each iteration, we stored a cache which contains (X,W,b,z). In the back prop module, we will use this cache to compute the gradients.\n",
    "\n",
    "The first thing we need to do before backpropagating is to compute the first gradient $ dA^{[L]}= \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}}$ = dAL where $A^{[L]} = \\sigma(Z^{[L]})$ :\n",
    "\n",
    "$$dA^{[L]} = \\frac{\\partial \\mathcal{L}}{\\partial A^{[L]}} = \\frac{Y}{ A^{[L]}} - \\frac{1-Y}{1-A^{[L]}}$$\n",
    "\n",
    "We can then use dAL to keep going backward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: L_model_backward\n",
    "\n",
    "def L_model_backward(AL, Y, caches):\n",
    "    \"\"\"\n",
    "    Implement the backward propagation for the [LINEAR->RELU] * (L-1) -> LINEAR -> SIGMOID group\n",
    "    \n",
    "    Arguments:\n",
    "    AL -- probability vector, output of the forward propagation (L_model_forward())\n",
    "    Y -- true \"label\" vector (containing 0 if non-cat, 1 if cat)\n",
    "    caches -- list of caches containing:\n",
    "                every cache of linear_activation_forward() with \"relu\" (it's caches[l], for l in range(L-1) i.e l = 0...L-2)\n",
    "                the cache of linear_activation_forward() with \"sigmoid\" (it's caches[L-1])\n",
    "    \n",
    "    Returns:\n",
    "    grads -- A dictionary with the gradients\n",
    "             grads[\"dA\" + str(l)] = ... \n",
    "             grads[\"dW\" + str(l)] = ...\n",
    "             grads[\"db\" + str(l)] = ... \n",
    "    \"\"\"\n",
    "    grads = {}\n",
    "    L = len(caches) # the number of layers\n",
    "    m = AL.shape[1]\n",
    "    Y = Y.reshape(AL.shape) # after this line, Y is the same shape as AL\n",
    "    \n",
    "    # Initializing the backpropagation\n",
    "    dAL = -(np.divide(Y, AL) - np.divide(1-Y, 1-AL))\n",
    "    \n",
    "    # Lth layer (SIGMOID -> LINEAR) gradients. Inputs: \"AL, Y, caches\". Outputs: \"grads[\"dAL\"], grads[\"dWL\"], grads[\"dbL\"]\n",
    "    grads[\"dA\" + str(L)], grads[\"dW\" + str(L)], grads[\"db\" + str(L)] = linear_activation_backward(dAL, caches[-1], activation=\"sigmoid\")\n",
    "    \n",
    "    for l in reversed(range(L-1)):\n",
    "        # lth layer: (RELU -> LINEAR) gradients.\n",
    "        # Inputs: \"grads[\"dA\" + str(l + 2)], caches\". Outputs: \"grads[\"dA\" + str(l + 1)] , grads[\"dW\" + str(l + 1)] , grads[\"db\" + str(l + 1)] \n",
    "        grads[\"dA\" + str(l + 1)], grads[\"dW\" + str(l + 1)], grads[\"db\" + str(l + 1)] = linear_activation_backward(grads[\"dA\" + str(l + 2)], caches[l], activation=\"relu\")\n",
    "\n",
    "    return grads\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dW1 = [[0.41010002 0.07807203 0.13798444 0.10502167]\n",
      " [0.         0.         0.         0.        ]\n",
      " [0.05283652 0.01005865 0.01777766 0.0135308 ]]\n",
      "db1 = [[-0.22007063]\n",
      " [ 0.        ]\n",
      " [-0.02835349]]\n",
      "dA1 = [[ 0.          0.52257901]\n",
      " [ 0.         -0.3269206 ]\n",
      " [ 0.         -0.32070404]\n",
      " [ 0.         -0.74079187]]\n"
     ]
    }
   ],
   "source": [
    "AL, Y_assess, caches = L_model_backward_test_case()\n",
    "grads = L_model_backward(AL, Y_assess, caches)\n",
    "print (\"dW1 = \"+ str(grads[\"dW1\"]))\n",
    "print (\"db1 = \"+ str(grads[\"db1\"]))\n",
    "print (\"dA1 = \"+ str(grads[\"dA1\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Update parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The hardest is done, we just need to update our parameters, and we are DONE with the building blocks, then we will be able to put everything together, train our model and make predictions (let's gooooooooo).\n",
    "\n",
    "We update our params like dat :\n",
    "\n",
    "$$ W^{[l]} = W^{[l]} - \\alpha \\text{ } dW^{[l]}$$\n",
    "$$ b^{[l]} = b^{[l]} - \\alpha \\text{ } db^{[l]}$$\n",
    "\n",
    "where $\\alpha$ is the learning rate. After computing the updated parameters, store them in the parameters dictionary. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def update_parameters(parameters, grads, learning_rate):\n",
    "    \"\"\"\n",
    "    Update parameters using gradient descent\n",
    "    \n",
    "    Arguments:\n",
    "    @param parameters - python dictionary containing the parameters \n",
    "    @param grads      - python dictionary containing the gradients, output of L_model_backward\n",
    "    \n",
    "    Returns:\n",
    "    @return parameters  - python dictionary containing the updated parameters \n",
    "                          parameters[\"W\" + str(l)] = ... \n",
    "                          parameters[\"b\" + str(l)] = ...\n",
    "    \"\"\"\n",
    "    \n",
    "    L = len(parameters) // 2 # number of layers in the neural network\n",
    "\n",
    "    for l in range(L):\n",
    "        parameters[\"W\" + str(l+1)] -= learning_rate*grads[\"dW\"+str(l+1)]  \n",
    "        parameters[\"b\" + str(l+1)] -= learning_rate*grads[\"db\"+str(l+1)]\n",
    "        \n",
    "    return parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "W1 = [[-0.59562069 -0.09991781 -2.14584584  1.82662008]\n",
      " [-1.76569676 -0.80627147  0.51115557 -1.18258802]\n",
      " [-1.0535704  -0.86128581  0.68284052  2.20374577]]\n",
      "b1 = [[-0.04659241]\n",
      " [-1.28888275]\n",
      " [ 0.53405496]]\n",
      "W2 = [[-0.55569196  0.0354055   1.32964895]]\n",
      "b2 = [[-0.84610769]]\n"
     ]
    }
   ],
   "source": [
    "parameters, grads = update_parameters_test_case()\n",
    "parameters = update_parameters(parameters, grads, 0.1)\n",
    "\n",
    "print (\"W1 = \"+ str(parameters[\"W1\"]))\n",
    "print (\"b1 = \"+ str(parameters[\"b1\"]))\n",
    "print (\"W2 = \"+ str(parameters[\"W2\"]))\n",
    "print (\"b2 = \"+ str(parameters[\"b2\"]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
